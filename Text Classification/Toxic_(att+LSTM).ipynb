{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic (att+LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcbRkWyCQJgP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras \n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx6ZHbOwQT4i"
      },
      "source": [
        "data=pd.read_csv('/content/drive/My Drive/dataset/ttran.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9IYTdrWQYpx"
      },
      "source": [
        "data.drop('id',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bc2siqxQbIt"
      },
      "source": [
        "llabel=[]\n",
        "llabel1=[]\n",
        "llabel2=[]\n",
        "llabel3=[]\n",
        "llabel4=[]\n",
        "llabel5=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ3p4EJQQq5P"
      },
      "source": [
        "llabel=list(data['toxic'])\n",
        "llabel1=list(data['severe_toxic'])\n",
        "llabel2=list(data['obscene'])\n",
        "llabel3=list(data['threat'])\n",
        "llabel4=list(data['insult'])\n",
        "llabel5=list(data['identity_hate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_F-P4-QsnS"
      },
      "source": [
        "labell=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZGcCFnZQuR-"
      },
      "source": [
        "for a,b,c,d,e,f in zip(llabel,llabel1,llabel2,llabel3,llabel4,llabel5):\n",
        "  labell.append([a,b,c,d,e,f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqF8BUUWQ4Hv"
      },
      "source": [
        "import re\n",
        "def preprocess_sentence(w):\n",
        "  w = w.lower()\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__O8WBeTQ6eQ"
      },
      "source": [
        "texts=[]\n",
        "a=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5iNNMyTQ77a"
      },
      "source": [
        "for line in data['comment_text']:\n",
        "  a=preprocess_sentence(line)\n",
        "  texts.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEVGCV7AQ_cg"
      },
      "source": [
        "from keras.preprocessing import text, sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ2UstgLRGkh"
      },
      "source": [
        "max_features = 30000\n",
        "maxlen = 100\n",
        "embed_size = 300\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(texts)\n",
        "x_train1 = sequence.pad_sequences(X_train, maxlen=maxlen,padding=\"post\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71W5rSTVCtR"
      },
      "source": [
        "b=tf.keras.layers.LSTM(256)\n",
        "dd=tf.keras.layers.Embedding(30000,256)\n",
        "d=tf.keras.layers.Dense(64)\n",
        "m=tf.keras.layers.Flatten()\n",
        "ccc=tf.keras.layers.GlobalMaxPool1D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuyHQM8ERWLH"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.models import Model\n",
        "from keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCTK3odvRYp-"
      },
      "source": [
        "class Position_Embedding(Layer):\n",
        "\n",
        "  def __init__(self, size=None, mode='sum', **kwargs):\n",
        "    self.size = size \n",
        "    self.mode = mode\n",
        "    super(Position_Embedding, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    if (self.size == None) or (self.mode == 'sum'):\n",
        "      self.size = int(x.shape[-1])\n",
        "    batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]\n",
        "    position_j = 1. / K.pow(10000., 2 * K.arange(self.size / 2, dtype='float32') / self.size)\n",
        "    position_j = K.expand_dims(position_j, 0)\n",
        "    position_i = K.cumsum(K.ones_like(x[:, :, 0]),1) - 1 \n",
        "    position_i = K.expand_dims(position_i, 2)\n",
        "    position_ij = K.dot(position_i, position_j)\n",
        "    position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
        "    if self.mode == 'sum':\n",
        "      return position_ij + x\n",
        "    elif self.mode == 'concat':\n",
        "      return K.concatenate([position_ij, x], 2)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if self.mode == 'sum':\n",
        "      return input_shape\n",
        "    elif self.mode == 'concat':\n",
        "      return (input_shape[0], input_shape[1], input_shape[2] + self.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHw8qv-MST-A"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
        "  output = tf.matmul(attention_weights, v)  \n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH7DMCqXT0H6"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    assert d_model % self.num_heads == 0\n",
        "    self.depth = d_model // self.num_heads\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    q = self.wq(q)  \n",
        "    k = self.wk(k)    \n",
        "    v = self.wv(v)    \n",
        "    q = self.split_heads(q, batch_size)  \n",
        "    k = self.split_heads(k, batch_size)  \n",
        "    v = self.split_heads(v, batch_size) \n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  \n",
        "    output = self.dense(concat_attention)  \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXVrm7vZSR4D"
      },
      "source": [
        "S_inputs = Input(shape=([None]), dtype='int32')\n",
        "embeddings = dd(S_inputs)\n",
        "\n",
        "c=b(embeddings)\n",
        "O_seq,a = MultiHeadAttention(512, 8)(c,c,c,None)\n",
        "b_seq=d(O_seq)\n",
        "e = Dropout(0.2)(b_seq)\n",
        "e=ccc(e)\n",
        "outputs = Dense(6, activation='softmax')(e)\n",
        "model = Model(inputs=S_inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL2TGttigxIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "8f9bf9dc-a1bc-4b9c-bc08-6f6492294ea2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    7680000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention (MultiHead ((None, None, 512),  657408      lstm[0][0]                       \n",
            "                                                                 lstm[0][0]                       \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 64)     32832       multi_head_attention[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 64)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 64)           0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 6)            390         global_max_pooling1d[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 8,895,942\n",
            "Trainable params: 8,895,942\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dz4CTSQVwC7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(\n",
        "    x_train1, labell, test_size=0.3, random_state=233)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cekg3ec6WcY1"
      },
      "source": [
        "X_tain1=tf.data.Dataset.from_tensor_slices((X_tra,y_tra))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IekbWRLeWmex"
      },
      "source": [
        "X_val1=tf.data.Dataset.from_tensor_slices((X_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXQ9QfAaWq8h"
      },
      "source": [
        "XX_t=X_tain1.batch(32).shuffle(10000)\n",
        "XX_v=X_val1.batch(32).shuffle(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WneKoU_LbBzj"
      },
      "source": [
        "loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKJz73ELVW93"
      },
      "source": [
        "model.compile(loss=loss_fn, optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdlCwBVjWKt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ec92971-06a4-4f96-cd4c-1ea43ba763e4"
      },
      "source": [
        "hist = model.fit(XX_t,batch_size=32,epochs=50,validation_data=(XX_v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3491/3491 [==============================] - 298s 85ms/step - loss: 0.3655 - accuracy: 0.9516 - val_loss: 0.3612 - val_accuracy: 0.9938\n",
            "Epoch 2/50\n",
            "3491/3491 [==============================] - 299s 86ms/step - loss: 0.3645 - accuracy: 0.8946 - val_loss: 0.3575 - val_accuracy: 0.5382\n",
            "Epoch 3/50\n",
            "3491/3491 [==============================] - 303s 87ms/step - loss: 0.3620 - accuracy: 0.7956 - val_loss: 0.3553 - val_accuracy: 0.8988\n",
            "Epoch 4/50\n",
            " 360/3491 [==>...........................] - ETA: 4:22 - loss: 0.3576 - accuracy: 0.6422"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-26-020369656f56>\", line 1, in <module>\n",
            "    hist = model.fit(XX_t,batch_size=32,epochs=50,validation_data=(XX_v))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\n",
            "    return method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\n",
            "    tmp_logs = train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 807, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\n",
            "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\n",
            "    cancellation_manager=cancellation_manager)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hw28aZLWYJe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}